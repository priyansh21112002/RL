{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3K6EGmsiNJz",
        "outputId": "1076865f-01c2-4357-8728-0b753a9204c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.24499386 0.22780564 0.22887017 0.22295258]\n",
            " [0.12371505 0.13193191 0.13893144 0.21043981]\n",
            " [0.22048782 0.18351757 0.17914378 0.18247866]\n",
            " [0.13273611 0.11067872 0.09951371 0.17347632]\n",
            " [0.27672594 0.20461735 0.12695699 0.09834951]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.25032858 0.11290035 0.11520803 0.05844781]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.17015976 0.22403019 0.26150006 0.3130842 ]\n",
            " [0.27925082 0.39987753 0.26224562 0.23145163]\n",
            " [0.53664619 0.36836008 0.30237019 0.26235564]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.35190928 0.28420828 0.46610933 0.42094468]\n",
            " [0.57218234 0.71668594 0.6832924  0.60694771]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# Set the parameters\n",
        "num_episodes = 10000\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "# Initialize the Q-table\n",
        "state_space = env.observation_space.n\n",
        "action_space = env.action_space.n\n",
        "q_table = np.zeros((state_space, action_space))\n",
        "\n",
        "# Function to choose an action based on epsilon-greedy strategy\n",
        "def choose_action(state):\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        # Explore: Choose a random action\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        # Exploit: Choose the action with the highest Q-value\n",
        "        return np.argmax(q_table[state, :])\n",
        "\n",
        "# SARSA algorithm\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    action = choose_action(state)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_action = choose_action(next_state)\n",
        "\n",
        "        # Update the Q-table using the SARSA update rule\n",
        "        q_table[state, action] += learning_rate * (\n",
        "                reward + discount_factor * q_table[next_state, next_action] - q_table[state, action])\n",
        "\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "\n",
        "# Print the learned Q-table\n",
        "print(q_table)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env=gym.make('FrozenLake-v1')\n",
        "\n",
        "epsilon=0.1\n",
        "alpha=0.1\n",
        "episodes=10000\n",
        "discount_factor=0.99\n",
        "\n",
        "state_space=env.observation_space.n\n",
        "action_space=env.action_space.n\n",
        "q_table=np.zeros((state_space,action_space))\n",
        "\n",
        "\n",
        "def choose_action(state):\n",
        "  if np.random.uniform(0,1)<epsilon:\n",
        "      return env.action_space.sample()\n",
        "\n",
        "  else:\n",
        "      return np.argmax(q_table[state,:])\n",
        "\n",
        "\n",
        "for episode in range(episodes):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  action=choose_action(state)\n",
        "\n",
        "  while not done:\n",
        "    \n",
        "    new_state,reward,done,_=env.step(action)\n",
        "    next_action=choose_action(new_state)\n",
        "\n",
        "    q_table[state,action]+=alpha*(reward+discount_factor*q_table[new_state,next_action]-q_table[state,action])\n",
        "\n",
        "    state=new_state\n",
        "    action=next_action\n",
        "\n",
        "print(q_table)\n",
        "\n",
        "env.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpJoBD2niNrj",
        "outputId": "c3e18642-5dcc-4231-8774-a1f67100701f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.37138547 0.33082017 0.33505441 0.31308459]\n",
            " [0.17612233 0.16646817 0.17358049 0.28861712]\n",
            " [0.23502982 0.17206908 0.14705498 0.16744434]\n",
            " [0.12954693 0.00904905 0.         0.00117823]\n",
            " [0.41477323 0.26854221 0.30240712 0.19181916]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.14615186 0.23189741 0.12662986 0.0660602 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.2788914  0.30305724 0.30974364 0.45726182]\n",
            " [0.27153414 0.55436743 0.39370523 0.33674984]\n",
            " [0.57472091 0.43229217 0.40131662 0.2424928 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.3333029  0.56180558 0.64691909 0.25584236]\n",
            " [0.6016216  0.83573275 0.73392463 0.6214035 ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BttfyLiWdZKx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}